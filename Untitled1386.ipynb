{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "928386c2-d4f1-4a99-b484-d9316d3afa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] No history CSV at C:\\Users\\sagni\\Downloads\\Med Assist\\medassist_history.csv. Creating a small scratch curve plot (numeric-only, capped rows)...\n",
      "[INFO] Loaded C:\\Users\\sagni\\Downloads\\Med Assist\\archive\\mturkfitbit_export_4.12.16-5.12.16\\Fitabase Data 4.12.16-5.12.16 -> shape=(48438, 30)\n",
      "[INFO] Loaded C:\\Users\\sagni\\Downloads\\Med Assist\\archive\\mturkfitbit_export_3.12.16-4.11.16\\Fitabase Data 3.12.16-4.11.16 -> shape=(48658, 25)\n",
      "[OK] Saved scratch curves -> C:\\Users\\sagni\\Downloads\\Med Assist\\medassist_loss_mae.png\n",
      "[INFO] Loaded C:\\Users\\sagni\\Downloads\\Med Assist\\archive\\mturkfitbit_export_4.12.16-5.12.16\\Fitabase Data 4.12.16-5.12.16 -> shape=(48438, 30)\n",
      "[INFO] Loaded C:\\Users\\sagni\\Downloads\\Med Assist\\archive\\mturkfitbit_export_3.12.16-4.11.16\\Fitabase Data 3.12.16-4.11.16 -> shape=(48658, 25)\n",
      "[OK] Saved Pred vs Actual heatmap -> C:\\Users\\sagni\\Downloads\\Med Assist\\medassist_pred_vs_actual.png\n",
      "[INFO] Loaded C:\\Users\\sagni\\Downloads\\Med Assist\\archive\\mturkfitbit_export_4.12.16-5.12.16\\Fitabase Data 4.12.16-5.12.16 -> shape=(48438, 30)\n",
      "[INFO] Loaded C:\\Users\\sagni\\Downloads\\Med Assist\\archive\\mturkfitbit_export_3.12.16-4.11.16\\Fitabase Data 3.12.16-4.11.16 -> shape=(48658, 25)\n",
      "[OK] Saved correlation heatmap -> C:\\Users\\sagni\\Downloads\\Med Assist\\medassist_corr_heatmap.png\n",
      "\n",
      "[DONE] Figures saved to:\n",
      " - C:\\Users\\sagni\\Downloads\\Med Assist\\medassist_loss_mae.png\n",
      " - C:\\Users\\sagni\\Downloads\\Med Assist\\medassist_pred_vs_actual.png\n",
      " - C:\\Users\\sagni\\Downloads\\Med Assist\\medassist_corr_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "# ==============================\n",
    "#            PATHS\n",
    "# ==============================\n",
    "BASE_DIR = r\"C:\\Users\\sagni\\Downloads\\Med Assist\"\n",
    "FOLDER1  = os.path.join(BASE_DIR, r\"archive\\mturkfitbit_export_4.12.16-5.12.16\\Fitabase Data 4.12.16-5.12.16\")\n",
    "FOLDER2  = os.path.join(BASE_DIR, r\"archive\\mturkfitbit_export_3.12.16-4.11.16\\Fitabase Data 3.12.16-4.11.16\")\n",
    "\n",
    "PKL_PATH = os.path.join(BASE_DIR, \"medassist_preprocess.pkl\")\n",
    "H5_PATH  = os.path.join(BASE_DIR, \"medassist_model.h5\")\n",
    "HIST_CSV = os.path.join(BASE_DIR, \"medassist_history.csv\")  # optional\n",
    "\n",
    "OUT_LOSS_MAE = os.path.join(BASE_DIR, \"medassist_loss_mae.png\")\n",
    "OUT_PVA      = os.path.join(BASE_DIR, \"medassist_pred_vs_actual.png\")\n",
    "OUT_CORR     = os.path.join(BASE_DIR, \"medassist_corr_heatmap.png\")\n",
    "\n",
    "# ==============================\n",
    "#     MEMORY & LOADING OPTS\n",
    "# ==============================\n",
    "INCLUDE_MINUTE_FILES = False\n",
    "ROW_LIMIT_PER_CSV    = None           # e.g. 150_000 to hard-cap rows per CSV\n",
    "SAMPLE_FRAC_PER_CSV  = 0.15\n",
    "BIG_FILE_MIN_ROWS    = 200_000\n",
    "RANDOM_STATE         = 42\n",
    "\n",
    "# Small scratch training (only if no history CSV is found)\n",
    "SCRATCH_EPOCHS   = 6\n",
    "SCRATCH_VAL_SIZE = 0.2\n",
    "SCRATCH_BATCH    = 64\n",
    "SCRATCH_MAX_ROWS = 10_000  # numeric-only, tiny to avoid OOM\n",
    "\n",
    "# Batch size for sparse->dense prediction\n",
    "PRED_BATCH = 2048          # small enough to avoid RAM spikes\n",
    "\n",
    "# Heuristics for file filtering\n",
    "INCLUDE_PATTERNS = [\n",
    "    \"daily\", \"day\", \"Sleep\", \"sleep\", \"weight\", \"Weight\",\n",
    "    \"summary\", \"calories\", \"Calories\", \"activities\",\n",
    "    \"heart\", \"resting\", \"steps\", \"Steps\", \"distance\", \"Distance\"\n",
    "]\n",
    "EXCLUDE_PATTERNS = [\"minute\", \"Minute\", \"seconds\", \"second\", \"intraday\", \"Intraday\"]\n",
    "\n",
    "# Guess target candidates\n",
    "TARGET_CANDIDATES = [\"Calories\", \"calories\", \"TotalCalories\", \"Calories Burned\", \"Calories_Burned\"]\n",
    "\n",
    "# ==============================\n",
    "#      BASIC HELPERS\n",
    "# ==============================\n",
    "def ensure_exists(p: str, name: str):\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"{name} not found at: {p}\")\n",
    "\n",
    "def _downcast_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in df.select_dtypes(include=[\"float64\"]).columns:\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "    for c in df.select_dtypes(include=[\"int64\"]).columns:\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "def _safe_union_concat(dfs):\n",
    "    if not dfs:\n",
    "        return pd.DataFrame()\n",
    "    cols = set()\n",
    "    for d in dfs:\n",
    "        cols.update(d.columns.tolist())\n",
    "    cols = list(cols)\n",
    "    aligned = [d.reindex(columns=cols) for d in dfs]\n",
    "    return pd.concat(aligned, ignore_index=True)\n",
    "\n",
    "def _want_file(fname: str) -> bool:\n",
    "    low = fname.lower()\n",
    "    if not INCLUDE_MINUTE_FILES:\n",
    "        for bad in EXCLUDE_PATTERNS:\n",
    "            if bad.lower() in low:\n",
    "                return False\n",
    "    for good in INCLUDE_PATTERNS:\n",
    "        if good.lower() in low:\n",
    "            return True\n",
    "    return INCLUDE_MINUTE_FILES\n",
    "\n",
    "def _read_csv_safely(path: str, row_limit):\n",
    "    try:\n",
    "        if row_limit:\n",
    "            df = pd.read_csv(path, nrows=row_limit)\n",
    "        else:\n",
    "            df = pd.read_csv(path)\n",
    "        return _downcast_numeric(df)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to read {path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def _maybe_sample(df: pd.DataFrame, frac: float) -> pd.DataFrame:\n",
    "    if frac is None or frac >= 1.0 or len(df) == 0:\n",
    "        return df\n",
    "    if len(df) >= BIG_FILE_MIN_ROWS:\n",
    "        df = df.sample(frac=frac, random_state=RANDOM_STATE)\n",
    "    return df\n",
    "\n",
    "def load_fitbit_folder(folder: str) -> pd.DataFrame:\n",
    "    if not os.path.isdir(folder):\n",
    "        print(f\"[WARN] Folder not found: {folder}\")\n",
    "        return pd.DataFrame()\n",
    "    csvs = [f for f in os.listdir(folder) if f.lower().endswith(\".csv\")]\n",
    "    kept = [f for f in csvs if _want_file(f)]\n",
    "    if not kept:\n",
    "        print(\"[WARN] No CSVs selected to load after filtering.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    dfs, total = [], 0\n",
    "    for i, fname in enumerate(kept, 1):\n",
    "        fpath = os.path.join(folder, fname)\n",
    "        df = _read_csv_safely(fpath, ROW_LIMIT_PER_CSV)\n",
    "        if df.empty:\n",
    "            continue\n",
    "        if ROW_LIMIT_PER_CSV is None:\n",
    "            df = _maybe_sample(df, SAMPLE_FRAC_PER_CSV)\n",
    "        df[\"__source_file\"] = fname\n",
    "        dfs.append(df)\n",
    "        total += len(df)\n",
    "        if len(dfs) >= 8:\n",
    "            dfs = [_safe_union_concat(dfs)]\n",
    "        if i % 10 == 0:\n",
    "            print(f\"[INFO] Loaded ~{i} files; rows so far: {total:,}\")\n",
    "    out = _safe_union_concat(dfs)\n",
    "    print(f\"[INFO] Loaded {folder} -> shape={out.shape}\")\n",
    "    return out\n",
    "\n",
    "def drop_ids_dates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    bads = []\n",
    "    for c in df.columns:\n",
    "        cl = c.lower()\n",
    "        if \"id\" in cl or \"date\" in cl or \"time\" in cl or \"datetime\" in cl:\n",
    "            bads.append(c)\n",
    "    return df.drop(columns=bads, errors=\"ignore\")\n",
    "\n",
    "def pick_target(df: pd.DataFrame) -> str:\n",
    "    for c in TARGET_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    nums = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if not nums:\n",
    "        raise ValueError(\"No numeric target column detected.\")\n",
    "    return nums[-1]\n",
    "\n",
    "def sparse_batch_predict(model, X, batch_size=2048, dtype=np.float32):\n",
    "    \"\"\"Predict in RAM-safe batches from (possibly sparse) X.\"\"\"\n",
    "    n = X.shape[0]\n",
    "    out = np.empty(n, dtype=np.float32)\n",
    "    for start in range(0, n, batch_size):\n",
    "        stop = min(start + batch_size, n)\n",
    "        Xi = X[start:stop]\n",
    "        if hasattr(Xi, \"toarray\"):\n",
    "            Xi = Xi.toarray().astype(dtype, copy=False)\n",
    "        else:\n",
    "            Xi = np.asarray(Xi, dtype=dtype)\n",
    "        out[start:stop] = model.predict(Xi, verbose=0).ravel().astype(np.float32)\n",
    "    return out\n",
    "\n",
    "# ==============================\n",
    "#   1) TRAINING CURVES PLOT\n",
    "# ==============================\n",
    "def plot_loss_mae(history_csv: str, out_path: str):\n",
    "    if not os.path.exists(history_csv):\n",
    "        print(f\"[WARN] No history CSV at {history_csv}; skipping curve plot.\")\n",
    "        return\n",
    "    hist = pd.read_csv(history_csv)\n",
    "    epochs = np.arange(1, len(hist) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    if \"loss\" in hist.columns:    plt.plot(epochs, hist[\"loss\"], label=\"Train Loss\")\n",
    "    if \"val_loss\" in hist.columns:plt.plot(epochs, hist[\"val_loss\"], label=\"Val Loss\")\n",
    "    if \"mae\" in hist.columns:     plt.plot(epochs, hist[\"mae\"], label=\"Train MAE\")\n",
    "    if \"val_mae\" in hist.columns: plt.plot(epochs, hist[\"val_mae\"], label=\"Val MAE\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Value\")\n",
    "    plt.title(\"Training Curves (Loss & MAE)\")\n",
    "    plt.grid(alpha=0.3); plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=160)\n",
    "    plt.close()\n",
    "    print(f\"[OK] Saved curves -> {out_path}\")\n",
    "\n",
    "def plot_loss_mae_fallback_sample(out_path: str):\n",
    "    \"\"\"No history CSV? Build a tiny numeric-only scratch history to avoid OOM.\"\"\"\n",
    "    try:\n",
    "        d1 = load_fitbit_folder(FOLDER1)\n",
    "        d2 = load_fitbit_folder(FOLDER2)\n",
    "        df = _safe_union_concat([d1, d2])\n",
    "        if df.empty:\n",
    "            print(\"[WARN] No data for scratch history.\")\n",
    "            return\n",
    "\n",
    "        df = drop_ids_dates(df)\n",
    "        # numeric-only to avoid OHE explosion\n",
    "        num_df = df.select_dtypes(include=[np.number]).copy()\n",
    "        if num_df.shape[1] < 2:\n",
    "            print(\"[WARN] Not enough numeric columns for scratch history.\")\n",
    "            return\n",
    "\n",
    "        # target guess: last numeric col\n",
    "        target_col = pick_target(num_df)\n",
    "        num_df = num_df.dropna(subset=[target_col]).copy()\n",
    "\n",
    "        # small sample\n",
    "        if len(num_df) > SCRATCH_MAX_ROWS:\n",
    "            num_df = num_df.sample(n=SCRATCH_MAX_ROWS, random_state=RANDOM_STATE)\n",
    "\n",
    "        y = num_df[target_col].astype(\"float32\").values\n",
    "        X = num_df.drop(columns=[target_col]).fillna(0.0).astype(\"float32\")\n",
    "\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=SCRATCH_VAL_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        Xtr = scaler.fit_transform(Xtr)\n",
    "        Xte = scaler.transform(Xte)\n",
    "\n",
    "        import tensorflow as tf\n",
    "        from tensorflow import keras\n",
    "        from tensorflow.keras import layers\n",
    "\n",
    "        model = keras.Sequential([\n",
    "            layers.Input(shape=(Xtr.shape[1],)),\n",
    "            layers.Dense(64, activation=\"relu\"),\n",
    "            layers.Dense(32, activation=\"relu\"),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "        hist = model.fit(\n",
    "            Xtr, ytr,\n",
    "            validation_data=(Xte, yte),\n",
    "            epochs=SCRATCH_EPOCHS,\n",
    "            batch_size=SCRATCH_BATCH,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(hist.history.get(\"loss\", []), label=\"Train Loss\")\n",
    "        plt.plot(hist.history.get(\"val_loss\", []), label=\"Val Loss\")\n",
    "        plt.plot(hist.history.get(\"mae\", []), label=\"Train MAE\")\n",
    "        plt.plot(hist.history.get(\"val_mae\", []), label=\"Val MAE\")\n",
    "        plt.xlabel(\"Epoch\"); plt.ylabel(\"Value\")\n",
    "        plt.title(\"Training Curves (Scratch Sample, Numeric Only)\")\n",
    "        plt.grid(alpha=0.3); plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_path, dpi=160)\n",
    "        plt.close()\n",
    "        print(f\"[OK] Saved scratch curves -> {out_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Scratch history failed: {e}\")\n",
    "\n",
    "# ==============================\n",
    "#  2) PRED vs ACTUAL HEATMAP\n",
    "# ==============================\n",
    "def plot_pred_vs_actual_heatmap(pkl_path: str, h5_path: str, out_path: str):\n",
    "    ensure_exists(pkl_path, \"preprocess PKL\")\n",
    "    ensure_exists(h5_path, \"model H5\")\n",
    "\n",
    "    bundle = joblib.load(pkl_path)\n",
    "    pre = bundle[\"preprocess\"]\n",
    "    target_col = bundle.get(\"target_col\", None)\n",
    "\n",
    "    # Load a manageable sample from folders\n",
    "    d1 = load_fitbit_folder(FOLDER1)\n",
    "    d2 = load_fitbit_folder(FOLDER2)\n",
    "    df = _safe_union_concat([d1, d2])\n",
    "    if df.empty:\n",
    "        print(\"[WARN] No data for Pred vs Actual plot.\")\n",
    "        return\n",
    "    df = drop_ids_dates(df)\n",
    "\n",
    "    if target_col is None:\n",
    "        target_col = pick_target(df)\n",
    "    df = df.dropna(subset=[target_col]).copy()\n",
    "\n",
    "    # keep sample to make prediction fast and memory-safe\n",
    "    MAX_ROWS_FOR_PVA = 20_000\n",
    "    if len(df) > MAX_ROWS_FOR_PVA:\n",
    "        df = df.sample(n=MAX_ROWS_FOR_PVA, random_state=RANDOM_STATE)\n",
    "\n",
    "    y = df[target_col].astype(\"float32\").values\n",
    "    X = df.drop(columns=[target_col])\n",
    "\n",
    "    # Transform (likely sparse)\n",
    "    Xp = pre.transform(X)\n",
    "\n",
    "    # Load model without compiling to avoid 'mse' deserialization mismatch\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    model = keras.models.load_model(h5_path, compile=False)\n",
    "\n",
    "    # RAM-safe batch predictions\n",
    "    yhat = sparse_batch_predict(model, Xp, batch_size=PRED_BATCH, dtype=np.float32)\n",
    "\n",
    "    # 2D histogram heatmap\n",
    "    plt.figure(figsize=(6.5, 6))\n",
    "    both = np.concatenate([y, yhat])\n",
    "    vmin, vmax = np.nanpercentile(both, [1, 99])\n",
    "    plt.hist2d(y, yhat, bins=60, range=[[vmin, vmax], [vmin, vmax]], cmap=\"viridis\")\n",
    "    plt.xlabel(\"Actual\"); plt.ylabel(\"Predicted\")\n",
    "    plt.title(\"Predicted vs Actual (Heatmap)\")\n",
    "    cbar = plt.colorbar(); cbar.set_label(\"Count\")\n",
    "    plt.plot([vmin, vmax], [vmin, vmax], ls=\"--\", lw=1, color=\"white\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=160)\n",
    "    plt.close()\n",
    "    print(f\"[OK] Saved Pred vs Actual heatmap -> {out_path}\")\n",
    "\n",
    "# ==============================\n",
    "#   3) CORRELATION HEATMAP\n",
    "# ==============================\n",
    "def plot_corr_heatmap(out_path: str):\n",
    "    # Use the same sampled data for correlation\n",
    "    d1 = load_fitbit_folder(FOLDER1)\n",
    "    d2 = load_fitbit_folder(FOLDER2)\n",
    "    df = _safe_union_concat([d1, d2])\n",
    "    if df.empty:\n",
    "        print(\"[WARN] No data for correlation heatmap.\")\n",
    "        return\n",
    "    df = drop_ids_dates(df)\n",
    "\n",
    "    num_df = df.select_dtypes(include=[np.number]).copy()\n",
    "    # remove constant columns\n",
    "    nunique = num_df.nunique()\n",
    "    keep = nunique[nunique > 1].index.tolist()\n",
    "    num_df = num_df[keep]\n",
    "\n",
    "    if num_df.shape[1] < 2:\n",
    "        print(\"[WARN] Not enough numeric cols for correlation heatmap.\")\n",
    "        return\n",
    "\n",
    "    # limit width for readability\n",
    "    MAX_COLS = 30\n",
    "    if num_df.shape[1] > MAX_COLS:\n",
    "        variances = num_df.var().sort_values(ascending=False)\n",
    "        top_cols = variances.head(MAX_COLS).index.tolist()\n",
    "        num_df = num_df[top_cols]\n",
    "\n",
    "    corr = num_df.corr(numeric_only=True)\n",
    "\n",
    "    plt.figure(figsize=(min(12, 0.6*corr.shape[1]+4), min(10, 0.6*corr.shape[0]+4)))\n",
    "    try:\n",
    "        import seaborn as sns\n",
    "        sns.heatmap(corr, cmap=\"viridis\", square=False, cbar=True)\n",
    "    except Exception:\n",
    "        im = plt.imshow(corr.values, cmap=\"viridis\", aspect=\"auto\")\n",
    "        plt.colorbar(im)\n",
    "        plt.xticks(range(corr.shape[1]), corr.columns, rotation=90)\n",
    "        plt.yticks(range(corr.shape[0]), corr.index)\n",
    "    plt.title(\"Correlation Heatmap (Numeric Features)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=180)\n",
    "    plt.close()\n",
    "    print(f\"[OK] Saved correlation heatmap -> {out_path}\")\n",
    "\n",
    "# ==============================\n",
    "#            RUN\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Curves\n",
    "    if os.path.exists(HIST_CSV):\n",
    "        plot_loss_mae(HIST_CSV, OUT_LOSS_MAE)\n",
    "    else:\n",
    "        print(f\"[INFO] No history CSV at {HIST_CSV}. Creating a small scratch curve plot (numeric-only, capped rows)...\")\n",
    "        plot_loss_mae_fallback_sample(OUT_LOSS_MAE)\n",
    "\n",
    "    # 2) Pred vs Actual heatmap (batch, RAM-safe)\n",
    "    plot_pred_vs_actual_heatmap(PKL_PATH, H5_PATH, OUT_PVA)\n",
    "\n",
    "    # 3) Correlation heatmap\n",
    "    plot_corr_heatmap(OUT_CORR)\n",
    "\n",
    "    print(\"\\n[DONE] Figures saved to:\")\n",
    "    print(\" -\", OUT_LOSS_MAE)\n",
    "    print(\" -\", OUT_PVA)\n",
    "    print(\" -\", OUT_CORR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc3ec84-f9f4-4cc8-8f98-a1f57278fb23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
