{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8ad537-e8c1-4e06-8c3f-34212d1ed9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded C:\\Users\\sagni\\Downloads\\Med Assist\\archive\\mturkfitbit_export_4.12.16-5.12.16\\Fitabase Data 4.12.16-5.12.16 -> shape=(48438, 30)\n",
      "[INFO] Loaded C:\\Users\\sagni\\Downloads\\Med Assist\\archive\\mturkfitbit_export_3.12.16-4.11.16\\Fitabase Data 3.12.16-4.11.16 -> shape=(48658, 25)\n",
      "\n",
      "=== Prediction Preview (first 10 rows) ===\n",
      " Calories  prediction\n",
      "     55.0   74.901382\n",
      "    137.0  135.074036\n",
      "    184.0  102.015648\n",
      "     56.0  104.622215\n",
      "    103.0  124.953102\n",
      "     82.0  119.581207\n",
      "     84.0  116.181900\n",
      "     82.0  124.672668\n",
      "     83.0   75.677811\n",
      "     46.0  101.693863\n",
      "\n",
      "[OK] Saved predictions -> C:\\Users\\sagni\\Downloads\\Med Assist\\medassist_predictions.csv\n",
      "[OK] Saved summary     -> C:\\Users\\sagni\\Downloads\\Med Assist\\medassist_prediction_summary.json\n",
      "\n",
      "Quick metrics on scored rows:\n",
      "   MAE: 52.3179\n",
      "  RMSE: 126.0931\n",
      "    R2: 0.9367\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ========= EDIT YOUR PATHS =========\n",
    "BASE_DIR = r\"C:\\Users\\sagni\\Downloads\\Med Assist\"\n",
    "\n",
    "FOLDER1  = os.path.join(BASE_DIR, r\"archive\\mturkfitbit_export_4.12.16-5.12.16\\Fitabase Data 4.12.16-5.12.16\")\n",
    "FOLDER2  = os.path.join(BASE_DIR, r\"archive\\mturkfitbit_export_3.12.16-4.11.16\\Fitabase Data 3.12.16-4.11.16\")\n",
    "\n",
    "PKL_PATH = os.path.join(BASE_DIR, \"medassist_preprocess.pkl\")\n",
    "H5_PATH  = os.path.join(BASE_DIR, \"medassist_model.h5\")\n",
    "\n",
    "OUT_PRED = os.path.join(BASE_DIR, \"medassist_predictions.csv\")\n",
    "OUT_SUMM = os.path.join(BASE_DIR, \"medassist_prediction_summary.json\")\n",
    "\n",
    "# ========= RUNTIME TUNING =========\n",
    "INCLUDE_MINUTE_FILES = False\n",
    "ROW_LIMIT_PER_CSV    = None          # or set to a number like 100000\n",
    "SAMPLE_FRAC_PER_CSV  = 0.12\n",
    "BIG_FILE_MIN_ROWS    = 200_000\n",
    "RANDOM_STATE         = 42\n",
    "\n",
    "# number of rows to score (RAM-safe)\n",
    "MAX_ROWS_FOR_PREDICT = 10_000\n",
    "BATCH_SIZE           = 1024\n",
    "\n",
    "# candidate ground-truth columns (if present weâ€™ll print metrics)\n",
    "TARGET_CANDIDATES = [\n",
    "    \"Calories\", \"calories\", \"TotalCalories\",\n",
    "    \"Calories Burned\", \"Calories_Burned\", \"target\", \"y\"\n",
    "]\n",
    "\n",
    "# ========= FILE FILTERS =========\n",
    "INCLUDE_PATTERNS = [\n",
    "    \"daily\", \"day\", \"sleep\", \"Sleep\", \"weight\", \"Weight\",\n",
    "    \"summary\", \"calories\", \"Calories\", \"activities\",\n",
    "    \"heart\", \"resting\", \"steps\", \"Steps\", \"distance\", \"Distance\"\n",
    "]\n",
    "EXCLUDE_PATTERNS = [\"minute\", \"Minute\", \"seconds\", \"second\", \"intraday\", \"Intraday\"]\n",
    "\n",
    "\n",
    "# ========= HELPERS =========\n",
    "def ensure_exists(p: str, name: str):\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"{name} not found at: {p}\")\n",
    "\n",
    "def _downcast_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in df.select_dtypes(include=[\"float64\"]).columns:\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "    for c in df.select_dtypes(include=[\"int64\"]).columns:\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "def _want_file(fname: str) -> bool:\n",
    "    low = fname.lower()\n",
    "    if not INCLUDE_MINUTE_FILES:\n",
    "        for bad in EXCLUDE_PATTERNS:\n",
    "            if bad.lower() in low:\n",
    "                return False\n",
    "    for good in INCLUDE_PATTERNS:\n",
    "        if good.lower() in low:\n",
    "            return True\n",
    "    return INCLUDE_MINUTE_FILES\n",
    "\n",
    "def _read_csv_safely(path: str, row_limit):\n",
    "    try:\n",
    "        if row_limit:\n",
    "            df = pd.read_csv(path, nrows=row_limit)\n",
    "        else:\n",
    "            df = pd.read_csv(path)\n",
    "        return _downcast_numeric(df)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to read {path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def _maybe_sample(df: pd.DataFrame, frac: float) -> pd.DataFrame:\n",
    "    if frac is None or frac >= 1.0 or len(df) == 0:\n",
    "        return df\n",
    "    if len(df) >= BIG_FILE_MIN_ROWS:\n",
    "        df = df.sample(frac=frac, random_state=RANDOM_STATE)\n",
    "    return df\n",
    "\n",
    "def _safe_union_concat(dfs):\n",
    "    if not dfs:\n",
    "        return pd.DataFrame()\n",
    "    cols = set()\n",
    "    for d in dfs:\n",
    "        cols.update(d.columns.tolist())\n",
    "    cols = list(cols)\n",
    "    aligned = [d.reindex(columns=cols) for d in dfs]\n",
    "    return pd.concat(aligned, ignore_index=True)\n",
    "\n",
    "def load_fitbit_folder(folder: str) -> pd.DataFrame:\n",
    "    if not os.path.isdir(folder):\n",
    "        print(f\"[WARN] Folder not found: {folder}\")\n",
    "        return pd.DataFrame()\n",
    "    csvs = [f for f in os.listdir(folder) if f.lower().endswith(\".csv\")]\n",
    "    kept = [f for f in csvs if _want_file(f)]\n",
    "    if not kept:\n",
    "        print(\"[WARN] No CSVs selected to load after filtering.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    dfs, total = [], 0\n",
    "    for i, fname in enumerate(kept, 1):\n",
    "        fpath = os.path.join(folder, fname)\n",
    "        df = _read_csv_safely(fpath, ROW_LIMIT_PER_CSV)\n",
    "        if df.empty:\n",
    "            continue\n",
    "        if ROW_LIMIT_PER_CSV is None:\n",
    "            df = _maybe_sample(df, SAMPLE_FRAC_PER_CSV)\n",
    "        df[\"__source_file\"] = fname\n",
    "        dfs.append(df)\n",
    "        total += len(df)\n",
    "        if len(dfs) >= 8:  # keep memory in check\n",
    "            dfs = [_safe_union_concat(dfs)]\n",
    "        if i % 10 == 0:\n",
    "            print(f\"[INFO] Loaded ~{i} files; rows so far: {total:,}\")\n",
    "    out = _safe_union_concat(dfs)\n",
    "    print(f\"[INFO] Loaded {folder} -> shape={out.shape}\")\n",
    "    return out\n",
    "\n",
    "def drop_ids_dates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    bads = []\n",
    "    for c in df.columns:\n",
    "        cl = c.lower()\n",
    "        if \"id\" in cl or \"date\" in cl or \"time\" in cl or \"datetime\" in cl:\n",
    "            bads.append(c)\n",
    "    return df.drop(columns=bads, errors=\"ignore\")\n",
    "\n",
    "def pick_target(df: pd.DataFrame, bundle_target: str | None) -> str | None:\n",
    "    if bundle_target and bundle_target in df.columns:\n",
    "        return bundle_target\n",
    "    for c in TARGET_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    # no target found is OK (pure inference)\n",
    "    return None\n",
    "\n",
    "# RAM-safe batch predict from (possibly sparse) matrix\n",
    "def sparse_batch_predict(model, X, batch_size=1024, dtype=np.float32):\n",
    "    n = X.shape[0]\n",
    "    out = np.empty(n, dtype=np.float32)\n",
    "    for start in range(0, n, batch_size):\n",
    "        stop = min(start + batch_size, n)\n",
    "        Xi = X[start:stop]\n",
    "        if hasattr(Xi, \"toarray\"):   # sparse\n",
    "            Xi = Xi.toarray().astype(dtype, copy=False)\n",
    "        else:\n",
    "            Xi = np.asarray(Xi, dtype=dtype)\n",
    "        out[start:stop] = model.predict(Xi, verbose=0).ravel().astype(np.float32)\n",
    "    return out\n",
    "\n",
    "# Force all OneHotEncoders to be sparse_output=True (and sparse=True if present)\n",
    "def force_sparse_onehot(preprocessor):\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "    def _patch_ohe(ohe):\n",
    "        if hasattr(ohe, \"sparse_output\"):\n",
    "            try:\n",
    "                ohe.sparse_output = True\n",
    "            except Exception:\n",
    "                pass\n",
    "        if hasattr(ohe, \"sparse\"):\n",
    "            try:\n",
    "                ohe.sparse = True\n",
    "            except Exception:\n",
    "                pass\n",
    "        return ohe\n",
    "\n",
    "    def _recurse(est):\n",
    "        if isinstance(est, Pipeline):\n",
    "            for _, step in est.steps:\n",
    "                _recurse(step)\n",
    "        elif isinstance(est, ColumnTransformer):\n",
    "            for _, trans, _ in est.transformers:\n",
    "                _recurse(trans)\n",
    "        else:\n",
    "            if est.__class__.__name__ == \"OneHotEncoder\" or isinstance(est, OneHotEncoder):\n",
    "                _patch_ohe(est)\n",
    "\n",
    "    _recurse(preprocessor)\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "# ========= MAIN: PREDICT & SHOW =========\n",
    "def main():\n",
    "    # sanity checks\n",
    "    ensure_exists(PKL_PATH, \"preprocess PKL\")\n",
    "    ensure_exists(H5_PATH,  \"model H5\")\n",
    "\n",
    "    # load bundle + preprocessor\n",
    "    bundle = joblib.load(PKL_PATH)\n",
    "    pre = bundle[\"preprocess\"]\n",
    "    bundle_target = bundle.get(\"target_col\", None)\n",
    "\n",
    "    # force OHE sparse output to avoid giant dense matrices\n",
    "    pre = force_sparse_onehot(pre)\n",
    "\n",
    "    # load data from both folders\n",
    "    d1 = load_fitbit_folder(FOLDER1)\n",
    "    d2 = load_fitbit_folder(FOLDER2)\n",
    "    df = _safe_union_concat([d1, d2])\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"[ERROR] No rows to predict on.\")\n",
    "        return\n",
    "\n",
    "    # keep only useful columns and limit rows (RAM-safe)\n",
    "    df = drop_ids_dates(df)\n",
    "    if len(df) > MAX_ROWS_FOR_PREDICT:\n",
    "        df = df.sample(n=MAX_ROWS_FOR_PREDICT, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "    # detect target if present\n",
    "    target_col = pick_target(df, bundle_target)\n",
    "    if target_col and target_col in df.columns:\n",
    "        df_infer = df.dropna(subset=[target_col]).copy()\n",
    "    else:\n",
    "        df_infer = df.copy()\n",
    "\n",
    "    # build X / y (if available)\n",
    "    y_true = None\n",
    "    if target_col and target_col in df_infer.columns:\n",
    "        y_true = df_infer[target_col].astype(\"float32\").values\n",
    "        X = df_infer.drop(columns=[target_col])\n",
    "    else:\n",
    "        X = df_infer\n",
    "\n",
    "    # transform features (stays sparse)\n",
    "    Xp = pre.transform(X)\n",
    "\n",
    "    # load Keras model without compiling (avoids 'mse' deserialization mismatch)\n",
    "    from tensorflow import keras\n",
    "    model = keras.models.load_model(H5_PATH, compile=False)\n",
    "\n",
    "    # batched predictions\n",
    "    y_pred = sparse_batch_predict(model, Xp, batch_size=BATCH_SIZE, dtype=np.float32)\n",
    "\n",
    "    # assemble output frame\n",
    "    out = df_infer.copy()\n",
    "    out[\"prediction\"] = y_pred\n",
    "\n",
    "    # if ground truth exists, compute quick metrics\n",
    "    summary = {\"rows_scored\": int(len(out))}\n",
    "    if y_true is not None and len(y_true) == len(y_pred):\n",
    "        diff = y_pred - y_true\n",
    "        mae = float(np.mean(np.abs(diff)))\n",
    "        mse = float(np.mean(diff ** 2))\n",
    "        rmse = float(np.sqrt(mse))\n",
    "        # simple R2 (using numpy)\n",
    "        ss_res = float(np.sum(diff ** 2))\n",
    "        ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2)) if len(y_true) > 1 else float(\"nan\")\n",
    "        r2 = float(1.0 - ss_res / ss_tot) if ss_tot not in (0.0, float(\"nan\")) else float(\"nan\")\n",
    "        summary.update({\"mae\": mae, \"mse\": mse, \"rmse\": rmse, \"r2\": r2})\n",
    "\n",
    "    # show a small preview in console\n",
    "    show_cols = []\n",
    "    # Prefer a couple of common signal columns if present\n",
    "    for c in [\"Steps\", \"Distance\", \"Calories\", \"HeartRate\", \"SleepMinutes\"]:\n",
    "        if c in out.columns:\n",
    "            show_cols.append(c)\n",
    "    # Always include prediction (and target if exists)\n",
    "    if target_col and target_col in out.columns:\n",
    "        show_cols = [target_col] + show_cols\n",
    "    show_cols = list(dict.fromkeys(show_cols))  # dedupe\n",
    "    if \"prediction\" not in show_cols:\n",
    "        show_cols.append(\"prediction\")\n",
    "\n",
    "    print(\"\\n=== Prediction Preview (first 10 rows) ===\")\n",
    "    print(out[show_cols].head(10).to_string(index=False))\n",
    "\n",
    "    # save outputs\n",
    "    out.to_csv(OUT_PRED, index=False)\n",
    "    with open(OUT_SUMM, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(f\"\\n[OK] Saved predictions -> {OUT_PRED}\")\n",
    "    print(f\"[OK] Saved summary     -> {OUT_SUMM}\")\n",
    "    if \"mae\" in summary:\n",
    "        print(\"\\nQuick metrics on scored rows:\")\n",
    "        for k in [\"mae\", \"rmse\", \"r2\"]:\n",
    "            print(f\"  {k.upper():>4}: {summary[k]:,.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3169b5dd-7c95-48da-9c87-22e4f6178553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
