{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "633fb18e-64a5-4706-ad99-bbda0345aee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Memory-aware loading...\n",
      "[INFO] Skipping 10 CSVs (likely minute/intraday): first 5 -> ['minuteStepsWide_merged.csv', 'minuteCaloriesNarrow_merged.csv', 'heartrate_seconds_merged.csv', 'minuteCaloriesWide_merged.csv', 'minuteIntensitiesNarrow_merged.csv']\n",
      "[INFO] Loaded folder: C:\\Users\\sagni\\Downloads\\Med Assist\\archive\\mturkfitbit_export_4.12.16-5.12.16\\Fitabase Data 4.12.16-5.12.16\n",
      "[INFO] Combined shape: (48438, 30)\n",
      "[INFO] Skipping 7 CSVs (likely minute/intraday): first 5 -> ['minuteCaloriesNarrow_merged.csv', 'heartrate_seconds_merged.csv', 'minuteIntensitiesNarrow_merged.csv', 'minuteSleep_merged.csv', 'hourlyIntensities_merged.csv']\n",
      "[INFO] Loaded folder: C:\\Users\\sagni\\Downloads\\Med Assist\\archive\\mturkfitbit_export_3.12.16-4.11.16\\Fitabase Data 3.12.16-4.11.16\n",
      "[INFO] Combined shape: (48658, 25)\n",
      "[INFO] Feature matrix: train=(38816, 1526), test=(9704, 1526)\n",
      "Epoch 1/20\n",
      "\u001b[1m1213/1213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 174836.2656 - mae: 135.3417 - val_loss: 22049.2598 - val_mae: 55.3535\n",
      "Epoch 2/20\n",
      "\u001b[1m1213/1213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 23042.1855 - mae: 56.2941 - val_loss: 19355.7363 - val_mae: 54.0563\n",
      "Epoch 3/20\n",
      "\u001b[1m1213/1213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 21004.8633 - mae: 55.0670 - val_loss: 18559.8027 - val_mae: 54.3680\n",
      "Epoch 4/20\n",
      "\u001b[1m1213/1213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 20979.7344 - mae: 54.2905 - val_loss: 17696.8574 - val_mae: 52.8075\n",
      "Epoch 5/20\n",
      "\u001b[1m1213/1213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 21426.7754 - mae: 55.6271 - val_loss: 17357.1152 - val_mae: 54.3831\n",
      "Epoch 6/20\n",
      "\u001b[1m1213/1213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 20188.3027 - mae: 54.7444 - val_loss: 17216.4492 - val_mae: 53.0832\n",
      "Epoch 7/20\n",
      "\u001b[1m1213/1213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 18186.9766 - mae: 52.2230 - val_loss: 17373.0820 - val_mae: 53.2388\n",
      "Epoch 8/20\n",
      "\u001b[1m1213/1213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 19300.2422 - mae: 54.4065 - val_loss: 17401.7754 - val_mae: 52.7175\n",
      "Epoch 9/20\n",
      "\u001b[1m1213/1213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 19286.0371 - mae: 52.5096 - val_loss: 16787.3887 - val_mae: 52.0063\n",
      "Epoch 10/20\n",
      "\u001b[1m1213/1213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 17276.9004 - mae: 51.7753 - val_loss: 17365.1914 - val_mae: 52.7780\n",
      "Epoch 11/20\n",
      "\u001b[1m1213/1213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 19506.8457 - mae: 54.0737 - val_loss: 16225.3701 - val_mae: 51.4402\n",
      "Epoch 12/20\n",
      "\u001b[1m1213/1213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 17747.7676 - mae: 52.2045 - val_loss: 16493.5215 - val_mae: 51.9571\n",
      "Epoch 13/20\n",
      "\u001b[1m1213/1213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 17904.4766 - mae: 52.2163 - val_loss: 16440.8203 - val_mae: 51.6621\n",
      "Epoch 14/20\n",
      "\u001b[1m1213/1213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 17070.3770 - mae: 51.5362 - val_loss: 17528.6914 - val_mae: 54.2055\n",
      "Epoch 15/20\n",
      "\u001b[1m1213/1213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 17726.8906 - mae: 51.4919 - val_loss: 16548.6387 - val_mae: 51.1443\n",
      "Epoch 16/20\n",
      "\u001b[1m1213/1213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 17334.8633 - mae: 51.8928 - val_loss: 16106.9053 - val_mae: 51.5859\n",
      "Epoch 17/20\n",
      "\u001b[1m1213/1213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 16753.1816 - mae: 51.5837 - val_loss: 16673.1172 - val_mae: 52.7943\n",
      "Epoch 18/20\n",
      "\u001b[1m1213/1213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 17178.3945 - mae: 51.1177 - val_loss: 16334.5840 - val_mae: 52.2197\n",
      "Epoch 19/20\n",
      "\u001b[1m1213/1213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 17316.9141 - mae: 51.3733 - val_loss: 16219.3916 - val_mae: 50.3965\n",
      "Epoch 20/20\n",
      "\u001b[1m1213/1213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 16680.1953 - mae: 51.1220 - val_loss: 16446.4297 - val_mae: 52.9035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved preprocess bundle -> C:\\Users\\sagni\\Downloads\\Med Assist\\medassist_preprocess.pkl\n",
      "[OK] Saved model (.h5) -> C:\\Users\\sagni\\Downloads\\Med Assist\\medassist_model.h5\n",
      "[WARN] Skipped YAML export: 'Sequential' object has no attribute 'to_yaml'\n",
      "[OK] Saved metadata (.json) -> C:\\Users\\sagni\\Downloads\\Med Assist\\medassist_metadata.json\n",
      "\n",
      "[DONE] Artifacts in: C:\\Users\\sagni\\Downloads\\Med Assist\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List, Optional, Dict, Iterable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "#          CONFIG\n",
    "# ==============================\n",
    "BASE_DIR = r\"C:\\Users\\sagni\\Downloads\\Med Assist\"\n",
    "FOLDER1  = os.path.join(BASE_DIR, r\"archive\\mturkfitbit_export_4.12.16-5.12.16\\Fitabase Data 4.12.16-5.12.16\")\n",
    "FOLDER2  = os.path.join(BASE_DIR, r\"archive\\mturkfitbit_export_3.12.16-4.11.16\\Fitabase Data 3.12.16-4.11.16\")\n",
    "\n",
    "OUT_PKL  = os.path.join(BASE_DIR, \"medassist_preprocess.pkl\")\n",
    "OUT_H5   = os.path.join(BASE_DIR, \"medassist_model.h5\")\n",
    "OUT_YAML = os.path.join(BASE_DIR, \"medassist_model.yaml\")\n",
    "OUT_JSON = os.path.join(BASE_DIR, \"medassist_metadata.json\")\n",
    "\n",
    "# Memory controls\n",
    "INCLUDE_MINUTE_FILES = False        # set True to include minute/intraday CSVs\n",
    "ROW_LIMIT_PER_CSV    = None         # e.g., 150_000 to cap rows per large CSV\n",
    "SAMPLE_FRAC_PER_CSV  = 0.15         # subsample fraction if ROW_LIMIT_PER_CSV is None (applied to \"big\" files only)\n",
    "BIG_FILE_MIN_ROWS    = 200_000      # treat files with >= this many rows as \"big\"\n",
    "\n",
    "# Column name patterns to **include** by default (daily/summaries)\n",
    "INCLUDE_PATTERNS = [\n",
    "    \"daily\", \"day\", \"Sleep\", \"sleep\", \"weight\", \"Weight\", \"minuteSleep\", \"summary\",\n",
    "    \"calories\", \"Calories\", \"activities\", \"heart\", \"resting\", \"steps\", \"Steps\", \"distance\", \"Distance\"\n",
    "]\n",
    "\n",
    "# Column/file name patterns to **exclude** (intraday/minute/second)\n",
    "EXCLUDE_PATTERNS = [\n",
    "    \"minute\", \"Minute\", \"seconds\", \"second\", \"intraday\", \"Intraday\"\n",
    "]\n",
    "\n",
    "# Target column guesses\n",
    "TARGET_CANDIDATES = [\n",
    "    \"Calories\", \"calories\", \"TotalCalories\", \"Calories Burned\", \"Calories_Burned\"\n",
    "]\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "EPOCHS       = 20\n",
    "BATCH_SIZE   = 32\n",
    "\n",
    "# ==============================\n",
    "#       LOADING HELPERS\n",
    "# ==============================\n",
    "def _want_file(fname: str) -> bool:\n",
    "    \"\"\"Decide whether to load a CSV based on config flags and patterns.\"\"\"\n",
    "    low = fname.lower()\n",
    "    if not INCLUDE_MINUTE_FILES:\n",
    "        for bad in EXCLUDE_PATTERNS:\n",
    "            if bad.lower() in low:\n",
    "                return False\n",
    "    # if we excluded minute files, still allow daily/summary\n",
    "    for good in INCLUDE_PATTERNS:\n",
    "        if good.lower() in low:\n",
    "            return True\n",
    "    # If no include pattern matched, allow anyway if not excluded\n",
    "    return INCLUDE_MINUTE_FILES  # load only when explicitly allowed\n",
    "\n",
    "def _downcast_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Downcast floats/ints to reduce memory.\"\"\"\n",
    "    for c in df.select_dtypes(include=[\"float64\"]).columns:\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "    for c in df.select_dtypes(include=[\"int64\"]).columns:\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "def _safe_union_concat(dfs: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"Concatenate with unioned columns to avoid huge reindex allocations.\"\"\"\n",
    "    if not dfs:\n",
    "        return pd.DataFrame()\n",
    "    cols = set()\n",
    "    for d in dfs:\n",
    "        cols.update(d.columns.tolist())\n",
    "    cols = list(cols)\n",
    "    aligned = [d.reindex(columns=cols) for d in dfs]\n",
    "    return pd.concat(aligned, ignore_index=True)\n",
    "\n",
    "def _read_csv_safely(path: str, row_limit: Optional[int]) -> pd.DataFrame:\n",
    "    \"\"\"Read CSV with dtype downcast and optional row cap.\"\"\"\n",
    "    try:\n",
    "        if row_limit:\n",
    "            df = pd.read_csv(path, nrows=row_limit)\n",
    "        else:\n",
    "            df = pd.read_csv(path)\n",
    "        df = _downcast_numeric(df)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to read {path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def _maybe_sample(df: pd.DataFrame, frac: float) -> pd.DataFrame:\n",
    "    if frac is None or frac >= 1.0 or len(df) == 0:\n",
    "        return df\n",
    "    # only sample if \"big\"\n",
    "    if len(df) >= BIG_FILE_MIN_ROWS:\n",
    "        df = df.sample(frac=frac, random_state=RANDOM_STATE)\n",
    "    return df\n",
    "\n",
    "def load_fitbit_folder(folder: str) -> pd.DataFrame:\n",
    "    \"\"\"Memory-aware loader for a Fitabase export folder.\"\"\"\n",
    "    if not os.path.isdir(folder):\n",
    "        print(f\"[WARN] Folder not found: {folder}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    csv_files = [f for f in os.listdir(folder) if f.lower().endswith(\".csv\")]\n",
    "    kept = [f for f in csv_files if _want_file(f)]\n",
    "    skipped = set(csv_files) - set(kept)\n",
    "    if skipped:\n",
    "        print(f\"[INFO] Skipping {len(skipped)} CSVs (likely minute/intraday): first 5 -> {list(skipped)[:5]}\")\n",
    "    if not kept:\n",
    "        print(\"[WARN] No CSVs selected to load after filtering.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    dfs: List[pd.DataFrame] = []\n",
    "    total_rows = 0\n",
    "    for i, fname in enumerate(kept, 1):\n",
    "        fpath = os.path.join(folder, fname)\n",
    "        # quick probe: if file is HUGE and no ROW_LIMIT, sample fractionally\n",
    "        row_cap = ROW_LIMIT_PER_CSV\n",
    "        if row_cap is None:\n",
    "            # peek rows cheaply to decide sampling\n",
    "            try:\n",
    "                # read just to get shape quickly (may still be expensive)\n",
    "                probe = pd.read_csv(fpath, nrows=1000)\n",
    "                # If there are many columns, union later can be heavy — still safe.\n",
    "            except Exception:\n",
    "                probe = None\n",
    "        df = _read_csv_safely(fpath, row_cap)\n",
    "        if df.empty:\n",
    "            continue\n",
    "        if row_cap is None:\n",
    "            df = _maybe_sample(df, SAMPLE_FRAC_PER_CSV)\n",
    "        df[\"__source_file\"] = fname\n",
    "        dfs.append(df)\n",
    "        total_rows += len(df)\n",
    "        # concat in small batches to keep memory in check\n",
    "        if len(dfs) >= 8:\n",
    "            dfs = [_safe_union_concat(dfs)]\n",
    "        if i % 10 == 0:\n",
    "            print(f\"[INFO] Loaded ~{i} files; current rows: {total_rows:,}\")\n",
    "\n",
    "    combined = _safe_union_concat(dfs)\n",
    "    print(f\"[INFO] Loaded folder: {folder}\")\n",
    "    print(f\"[INFO] Combined shape: {combined.shape}\")\n",
    "    return combined\n",
    "\n",
    "# ==============================\n",
    "#   TARGET SELECTION & CLEAN\n",
    "# ==============================\n",
    "def pick_target(df: pd.DataFrame) -> str:\n",
    "    for c in TARGET_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    # fallback: last numeric column\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if not num_cols:\n",
    "        raise ValueError(\"No numeric target column detected.\")\n",
    "    return num_cols[-1]\n",
    "\n",
    "def drop_obvious_ids_and_dates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    bads = []\n",
    "    for c in df.columns:\n",
    "        cl = c.lower()\n",
    "        if \"id\" in cl or \"date\" in cl or \"time\" in cl or \"datetime\" in cl:\n",
    "            bads.append(c)\n",
    "    return df.drop(columns=bads, errors=\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "#     PREPROCESS & TRAIN\n",
    "# ==============================\n",
    "def make_preprocessor(X: pd.DataFrame, target_col: str) -> ColumnTransformer:\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "    # sklearn compatibility: OneHotEncoder param is sparse_output in >=1.2, sparse in older\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", ohe)\n",
    "    ])\n",
    "    pre = ColumnTransformer([\n",
    "        (\"num\", num_pipe, num_cols),\n",
    "        (\"cat\", cat_pipe, cat_cols)\n",
    "    ])\n",
    "    return pre\n",
    "\n",
    "def build_model(input_dim: int) -> keras.Model:\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "# ==============================\n",
    "#           MAIN\n",
    "# ==============================\n",
    "def main():\n",
    "    print(\"[INFO] Memory-aware loading...\")\n",
    "    df1 = load_fitbit_folder(FOLDER1)\n",
    "    df2 = load_fitbit_folder(FOLDER2)\n",
    "    df  = _safe_union_concat([df1, df2])\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\"No data loaded. Adjust INCLUDE_MINUTE_FILES / patterns or verify folders.\")\n",
    "\n",
    "    # Clean\n",
    "    df = drop_obvious_ids_and_dates(df)\n",
    "    # Drop entirely empty columns (after union)\n",
    "    empty_cols = [c for c in df.columns if df[c].isna().all()]\n",
    "    if empty_cols:\n",
    "        df = df.drop(columns=empty_cols)\n",
    "\n",
    "    # Target\n",
    "    target_col = pick_target(df)\n",
    "    df = df.dropna(subset=[target_col]).copy()\n",
    "\n",
    "    # Prepare X/y\n",
    "    y = df[target_col].astype(\"float32\").values\n",
    "    X = df.drop(columns=[target_col])\n",
    "\n",
    "    # Build preprocess & transform\n",
    "    pre = make_preprocessor(X, target_col)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    "    )\n",
    "    Xtr = pre.fit_transform(X_train)\n",
    "    Xte = pre.transform(X_test)\n",
    "\n",
    "    # Ensure float32 for Keras and reduce memory\n",
    "    if hasattr(Xtr, \"toarray\"):\n",
    "        Xtr = Xtr.toarray()\n",
    "        Xte = Xte.toarray()\n",
    "    Xtr = Xtr.astype(\"float32\", copy=False)\n",
    "    Xte = Xte.astype(\"float32\", copy=False)\n",
    "\n",
    "    print(f\"[INFO] Feature matrix: train={Xtr.shape}, test={Xte.shape}\")\n",
    "    model = build_model(Xtr.shape[1])\n",
    "\n",
    "    # Train\n",
    "    hist = model.fit(\n",
    "        Xtr, y_train,\n",
    "        validation_data=(Xte, y_test),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # ==========================\n",
    "    #       SAVE ARTIFACTS\n",
    "    # ==========================\n",
    "    bundle = {\n",
    "        \"preprocess\": pre,\n",
    "        \"target_col\": target_col,\n",
    "        \"numeric_cols\": X.select_dtypes(include=[np.number]).columns.tolist(),\n",
    "        \"cat_cols\": X.select_dtypes(exclude=[np.number]).columns.tolist(),\n",
    "    }\n",
    "    joblib.dump(bundle, OUT_PKL)\n",
    "    print(\"[OK] Saved preprocess bundle ->\", OUT_PKL)\n",
    "\n",
    "    model.save(OUT_H5)\n",
    "    print(\"[OK] Saved model (.h5) ->\", OUT_H5)\n",
    "\n",
    "    # YAML (optional; not all TF builds keep to_yaml)\n",
    "    try:\n",
    "        yaml_txt = model.to_yaml()\n",
    "        with open(OUT_YAML, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(yaml_txt)\n",
    "        print(\"[OK] Saved model (.yaml) ->\", OUT_YAML)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipped YAML export: {e}\")\n",
    "\n",
    "    meta = {\n",
    "        \"base_dir\": BASE_DIR,\n",
    "        \"folders\": [FOLDER1, FOLDER2],\n",
    "        \"include_minute_files\": INCLUDE_MINUTE_FILES,\n",
    "        \"row_limit_per_csv\": ROW_LIMIT_PER_CSV,\n",
    "        \"sample_frac_per_csv\": SAMPLE_FRAC_PER_CSV,\n",
    "        \"target_col\": target_col,\n",
    "        \"train_rows\": int(len(y_train)),\n",
    "        \"test_rows\": int(len(y_test)),\n",
    "        \"input_dim\": int(Xtr.shape[1]),\n",
    "        \"final_val_mae\": float(hist.history[\"val_mae\"][-1]),\n",
    "        \"final_val_loss\": float(hist.history[\"val_loss\"][-1]),\n",
    "        \"sklearn_version\": __import__(\"sklearn\").__version__,\n",
    "        \"tensorflow_version\": tf.__version__,\n",
    "    }\n",
    "    with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "    print(\"[OK] Saved metadata (.json) ->\", OUT_JSON)\n",
    "\n",
    "    print(\"\\n[DONE] Artifacts in:\", BASE_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3119f860-5fba-4f26-9961-a0f7d701cfd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
